{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b72b1a26-8e70-4722-b863-8b000899f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "using StringEncodings\n",
    "using DataStructures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a05f570-3644-4b0a-8c41-47323b61a24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Union{OrderedDict{Tuple{Base.CodeUnits{UInt8, String}, Base.CodeUnits{UInt8, String}}, Tuple{Base.CodeUnits{UInt8, String}, Int64}}, OrderedDict{Tuple{Char, Char}, Int64}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenType = Union{Int, Char}\n",
    "byteVec = Base.CodeUnits{UInt8, String}\n",
    "mergeDict = Union{OrderedDict{Tuple{byteVec, byteVec}, Tuple{byteVec, Int}}, OrderedDict{Tuple{Char, Char}, Int}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61267c65-a34a-42d0-b209-fcdb8fbbfe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct BPETokenizer\n",
    "    mergeDict::mergeDict\n",
    "    vocabDict::OrderedDict{Int, byteVec}\n",
    "    type_::tokenType\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d03286a0-f595-49e7-8d16-865add813e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct TrainingParams\n",
    "    vocab_size::Int\n",
    "    n_merges::Int\n",
    "    null_token::String\n",
    "    null_byte::byteVec\n",
    "\n",
    "    function TrainingParams(vocab_size, n_merges, null_token)\n",
    "        null_byte = transcode(UInt8, string(null_token))\n",
    "        return new(vocab_size, n_merges, null_token, null_byte)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2851371e-f6c1-4611-a6ef-95ca180e6be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encode_without_merging (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiple dispatches of the encoding_without_merging function\n",
    "function encode_without_merging(input::String)::Vector{byteVec}\n",
    "    vec_byteVec = Vector{byteVec}()\n",
    "    text_tokens = input |> collect\n",
    "    for token in text_tokens\n",
    "        push!(vec_byteVec, transcode(UInt8, string(token))) \n",
    "    end\n",
    "    return vec_byteVec \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1af5c336-4724-4266-910d-2d0cea2c6d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_stats (generic function with 2 methods)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiple dispatches of the get_stats function\n",
    "function get_stats(ids::Vector{byteVec})::OrderedDict{Tuple{byteVec, byteVec}, Int}\n",
    "    stats = OrderedDict{Tuple{byteVec,byteVec}, Int}()\n",
    "    for i in 1:(length(ids) - 1)\n",
    "        pair = (ids[i], ids[i+1])\n",
    "        stats[pair] = get(stats, pair, 0) + 1\n",
    "    end\n",
    "    return stats\n",
    "end\n",
    "\n",
    "function get_stats(ids::Vector{Vector{UInt8}})::OrderedDict{Tuple{Vector{UInt8}, Vector{UInt8}}, Int}\n",
    "    stats = OrderedDict{Tuple{UInt8, UInt8}, Int}()\n",
    "    for i in 1:(length(ids) - 1)\n",
    "        pair = (ids[i], ids[i+1])\n",
    "        stats[pair] = get(stats, pair, 0) + 1\n",
    "    end\n",
    "    return stats\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "315231a4-ed07-4a33-bd64-d56f1fdcd83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_top_pair (generic function with 2 methods)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiple dispatches of the get_top_pair function\n",
    "function get_top_pair(stats::OrderedDict{Tuple{byteVec, byteVec}, Int})::Tuple{byteVec, byteVec}\n",
    "    top_val = maximum(values(stats))\n",
    "    if top_val > 1\n",
    "        for (key, val) in stats\n",
    "            if val == top_val\n",
    "                return key\n",
    "            end\n",
    "        end\n",
    "    else \n",
    "        null_code = transcode(UInt8, string(\"<NULL>\"))\n",
    "        return (null_code, null_code)\n",
    "    end\n",
    "end\n",
    "\n",
    "function get_top_pair(stats::Dict{Tuple{UInt8, UInt8}, Int})::Tuple{UInt8, UInt8}\n",
    "    top_val = maximum(values(stats))\n",
    "    if top_val > 1\n",
    "        for (key, val) in stats\n",
    "            if val == top_val\n",
    "                return key\n",
    "            end\n",
    "        end\n",
    "    else \n",
    "        return (-1, -1)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "640b5eca-22b5-46c3-a387-a8c1ca5106ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "merge_pair (generic function with 2 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiple dispatches of the merge_pair function\n",
    "function merge_pair(tokens::Vector{byteVec}, pair::Tuple{byteVec, byteVec}, new_token::byteVec)::Vector{byteVec}\n",
    "    newTokens = byteVec[]\n",
    "    i = 1\n",
    "    while i < length(tokens) - 1\n",
    "        if i < length(tokens) && pair[1] == tokens[i] && pair[2] == tokens[i+1]\n",
    "            push!(newTokens, new_token)\n",
    "            i += 2\n",
    "        else\n",
    "            push!(newTokens, tokens[i])\n",
    "            i += 1\n",
    "        end\n",
    "    end\n",
    "    return newTokens\n",
    "end \n",
    "\n",
    "function merge_pair(ids::Vector{Int}, pair::Tuple{Int, Int}, idx::Int)::Vector{Int}\n",
    "    newIds = Int[]\n",
    "    i = 1\n",
    "    while i < length(ids) - 1\n",
    "        if i < length(ids) && pair[1] == ids[i] && pair[2] == ids[i+1]\n",
    "            push!(newIds, idx)\n",
    "            i += 2\n",
    "        else\n",
    "            push!(newIds, ids[i])\n",
    "            i += 1\n",
    "        end\n",
    "    end\n",
    "    return newIds\n",
    "end \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1c2d40c-9655-4d35-bd85-b4bed7f83dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Base.CodeUnits{UInt8, String}:\n",
       " 0x3c\n",
       " 0x4e\n",
       " 0x55\n",
       " 0x4c\n",
       " 0x4c\n",
       " 0x3e"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcode(UInt8, string(\"<NULL>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab63fdda-1fd6-4d98-a04d-f6ce18589173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ã¿': Unicode U+00FF (category Ll: Letter, lowercase)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Char(255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "316e8ebc-53d1-46f5-b654-926fbf9d1e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "build_vocab (generic function with 2 methods)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiple dispatches for the build_vocab function\n",
    "function build_vocab(merge_dict::OrderedDict{Tuple{byteVec, byteVec}, Tuple{byteVec, Int}})::OrderedDict{Int, byteVec}\n",
    "    vocab = OrderedDict{Int, byteVec}()\n",
    "    for idx in 0:255\n",
    "        vocab[idx] = transcode(UInt8, string(Char(idx))) \n",
    "    end\n",
    "    for ((p0, p1), (merged_token, idx)) in merge_dict\n",
    "        vocab[idx] = merged_token\n",
    "    end\n",
    "    return vocab\n",
    "end\n",
    "\n",
    "function build_vocab(merge_dict::OrderedDict{Tuple{Int, Int}, Int})::OrderedDict{Int, String}\n",
    "    vocab = OrderedDict{Int, String}()\n",
    "    for idx in 0:255\n",
    "        vocab[idx] = Char(idx)\n",
    "    end\n",
    "    for ((p0, p1), idx) in merge_dict\n",
    "        vocab[idx] = vocab[p0] * vocab[p1]\n",
    "    end\n",
    "    return vocab\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "180ef08d-0ecd-40fc-827c-654fce69cb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiple dispatches for the train function\n",
    "function train(tokenizer::BPETokenizer, params::TrainingParams, tokens::Vector{byteVec})\n",
    "    merges = OrderedDict{Tuple{byteVec, byteVec}, Tuple{byteVec, Int}}()\n",
    "    n_merges = params.n_merges\n",
    "    for i in 1:n_merges\n",
    "        stats = get_stats(tokens)\n",
    "        top_pair = get_top_pair(stats)\n",
    "        if top_pair[1] == params.null_byte && top_pair[2] == params.null_byte\n",
    "            break\n",
    "        end\n",
    "        idx = 256 + i\n",
    "        new_token = transcode(UInt8, String(vcat(top_pair[1], top_pair[2])))\n",
    "        tokens = merge_pair(tokens, top_pair, new_token)\n",
    "        merges[top_pair] = (new_token, idx)\n",
    "        @info \"Merging $top_pair into a new token $new_token\"  \n",
    "    end\n",
    "    vocab = build_vocab(merges)\n",
    "    tokenizer.mergeDict = merges\n",
    "    tokenizer.vocabDict = vocab\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84323153-e146-4391-bab1-ed5a69d35ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encode_tokens (generic function with 2 methods)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiple dispatches of the encode_tokens function\n",
    "function encode_tokens(tokenizer::BPETokenizer, input::String, type_::Int)::Vector{byteVec}\n",
    "    tokens = encode_without_merging(input, type_)\n",
    "    merges = tokenizer.mergeDict\n",
    "    while length(tokens) >= 2\n",
    "        stats = get_stats(tokens)\n",
    "        pair = findmin([get(merges, p, Inf) for p in keys(stats)])\n",
    "        if pair[2] â keys(merges)\n",
    "            break\n",
    "        end\n",
    "        idx = merges[pair]\n",
    "        tokens = merge_pair(tokens, pair, idx)\n",
    "    end\n",
    "    return tokens\n",
    "end  \n",
    "\n",
    "function encode_tokens(tokenizer::BPETokenizer, input::String, type_::Char)::Vector{Int}\n",
    "    tokens = encode_without_merging(input, type_)\n",
    "    merges = tokenizer.mergeDict\n",
    "    while length(tokens) >= 2\n",
    "        stats = get_stats(tokens)\n",
    "        pair = findmin([get(merges, p, Inf) for p in keys(stats)])\n",
    "        if pair[2] â keys(merges)\n",
    "            break\n",
    "        end\n",
    "        idx = merges[pair]\n",
    "        tokens = merge_pair(tokens, pair, idx)\n",
    "    end\n",
    "    return tokens\n",
    "end  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2dc1924-3cdc-42b1-9009-aa1203795e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decode_tokens (generic function with 2 methods)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiple dispatches of decode_tokens function\n",
    "function decode_tokens(tokenizer::BPETokenizer, ids::Vector{Int}, type_::Char)::String\n",
    "    @assert typeof(type_) == Char \"Not type of Char\"\n",
    "    vocab = tokenizer.vocabDict\n",
    "    tokens = join([vocab[idx] for idx in ids])\n",
    "    return tokens\n",
    "end\n",
    "\n",
    "function decode_tokens(tokenizer::BPETokenizer, ids::Vector{Int}, type_::Int)::String\n",
    "    @assert typeof(type_) == Int64 \"Not type of Int64\"\n",
    "    vocab = tokenizer.vocabDict\n",
    "    tokens = join([vocab[idx] for idx in ids])\n",
    "    @show tokens\n",
    "    code_units = codeunits(tokens)\n",
    "    code_points = [Char(code_unit) for code_unit in code_units]\n",
    "    text = String(code_points)\n",
    "    return text\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b12a26cb-6ea9-4765-bfb8-9efce54556e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"æ®éã®è¨èã¨ç°¡åãªæç« ãä½¿ã£ã¦æ¸ãããã«ãã¦ãã¾ãã\\n\\nãã®ç¨®ã®æç« ã¯èª­ã¿ããããèª­ã¿ããããã®ã§ããã°ããã»ã©ãèª­èã¯ããæ·±ããã®åå®¹ã«å¼ãè¾¼ã¾ãã¾ããå½¼ããããªãã®æ£æã«è²»ããã¨ãã«ã®ã¼ãæ¸ãã°æ¸ãã»ã©ãããªãã®ã¢ã¤ãã¢ã®ããã«å¤ãã®æéãæ®ãããã§ãããã\\nà®µà®£à®à¯à®à®®à¯, à®¨à¯à®à¯à®à®³à¯ à®à®ªà¯à®ªà®à®¿ à®à®°à¯à®à¯à®à®¿à®±à¯à®°à¯à®à®³à¯ ? I try to write using ordinary words and simple sentences.\\n\\nThat kind of writing is easier to read\"\u001b[93m\u001b[1m â¯ 2417 bytes â¯ \u001b[22m\u001b[39m\"uld never do it by accident.\\n\\nThe other reason my writing ends up being simple is the way I do it. I write the first draft fast, then spend days editing it, trying to get everything just right. Much of this editing is cutting, and that makes simple writing even simpler.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = \"\"\"æ®éã®è¨èã¨ç°¡åãªæç« ãä½¿ã£ã¦æ¸ãããã«ãã¦ãã¾ãã\n",
    "\n",
    "ãã®ç¨®ã®æç« ã¯èª­ã¿ããããèª­ã¿ããããã®ã§ããã°ããã»ã©ãèª­èã¯ããæ·±ããã®åå®¹ã«å¼ãè¾¼ã¾ãã¾ããå½¼ããããªãã®æ£æã«è²»ããã¨ãã«ã®ã¼ãæ¸ãã°æ¸ãã»ã©ãããªãã®ã¢ã¤ãã¢ã®ããã«å¤ãã®æéãæ®ãããã§ãããã\n",
    "à®µà®£à®à¯à®à®®à¯, à®¨à¯à®à¯à®à®³à¯ à®à®ªà¯à®ªà®à®¿ à®à®°à¯à®à¯à®à®¿à®±à¯à®°à¯à®à®³à¯ ? I try to write using ordinary words and simple sentences.\n",
    "\n",
    "That kind of writing is easier to read, and the easier something is to read, the more deeply readers will engage with it. The less energy they expend on your prose, the more they'll have left for your ideas.\n",
    "\n",
    "And the further they'll read. Most readers' energy tends to flag part way through an article or essay. If the friction of reading is low enough, more keep going till the end.\n",
    "\n",
    "There's an Italian dish called saltimbocca, which means \"leap into the mouth.\" My goal when writing might be called saltintesta: the ideas leap into your head and you barely notice the words that got them there.\n",
    "\n",
    "It's too much to hope that writing could ever be pure ideas. You might not even want it to be. But for most writers, most of the time, that's the goal to aim for. The gap between most writing and pure ideas is not filled with poetry.\n",
    "\n",
    "Plus it's more considerate to write simply. When you write in a fancy way to impress people, you're making them do extra work just so you can seem cool. It's like trailing a long train behind you that readers have to carry.\n",
    "\n",
    "And remember, if you're writing in English, that a lot of your readers won't be native English speakers. Their understanding of ideas may be way ahead of their understanding of English. So you can't assume that writing about a difficult topic means you can use difficult words.\n",
    "\n",
    "Of course, fancy writing doesn't just conceal ideas. It can also conceal the lack of them. That's why some people write that way, to conceal the fact that they have nothing to say. Whereas writing simply keeps you honest. If you say nothing simply, it will be obvious to everyone, including you.\n",
    "\n",
    "Simple writing also lasts better. People reading your stuff in the future will be in much the same position as people from other countries reading it today. The culture and the language will have changed. It's not vain to care about that, any more than it's vain for a woodworker to build a chair to last.\n",
    "\n",
    "Indeed, lasting is not merely an accidental quality of chairs, or writing. It's a sign you did a good job.\n",
    "\n",
    "But although these are all real advantages of writing simply, none of them are why I do it. The main reason I write simply is that it offends me not to. When I write a sentence that seems too complicated, or that uses unnecessarily intellectual words, it doesn't seem fancy to me. It seems clumsy.\n",
    "\n",
    "There are of course times when you want to use a complicated sentence or fancy word for effect. But you should never do it by accident.\n",
    "\n",
    "The other reason my writing ends up being simple is the way I do it. I write the first draft fast, then spend days editing it, trying to get everything just right. Much of this editing is cutting, and that makes simple writing even simpler.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85f80b5a-feae-4d09-ab0b-4eb4d64a08d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2957-element Vector{Base.CodeUnits{UInt8, String}}:\n",
       " [0xe6, 0x99, 0xae]\n",
       " [0xe9, 0x80, 0x9a]\n",
       " [0xe3, 0x81, 0xae]\n",
       " [0xe8, 0xa8, 0x80]\n",
       " [0xe8, 0x91, 0x89]\n",
       " [0xe3, 0x81, 0xa8]\n",
       " [0xe7, 0xb0, 0xa1]\n",
       " [0xe5, 0x8d, 0x98]\n",
       " [0xe3, 0x81, 0xaa]\n",
       " [0xe6, 0x96, 0x87]\n",
       " [0xe7, 0xab, 0xa0]\n",
       " [0xe3, 0x82, 0x92]\n",
       " [0xe4, 0xbd, 0xbf]\n",
       " â®\n",
       " [0x76]\n",
       " [0x65]\n",
       " [0x6e]\n",
       " [0x20]\n",
       " [0x73]\n",
       " [0x69]\n",
       " [0x6d]\n",
       " [0x70]\n",
       " [0x6c]\n",
       " [0x65]\n",
       " [0x72]\n",
       " [0x2e]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_vec = encode_without_merging(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e496bc4-0082-42b1-a8ee-ab7fe522c999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BPETokenizer(OrderedDict{Tuple{Base.CodeUnits{UInt8, String}, Base.CodeUnits{UInt8, String}}, Tuple{Base.CodeUnits{UInt8, String}, Int64}}(), OrderedDict{Int64, Base.CodeUnits{UInt8, String}}(), ' ')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BPETokenizer(OrderedDict{Tuple{byteVec, byteVec}, Tuple{byteVec, Int}}(), OrderedDict{Int, byteVec}(), Char(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6febf871-abbf-49ad-92f9-2be11df9d2f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingParams(300, 44, \"<NULL>\", UInt8[0x3c, 0x4e, 0x55, 0x4c, 0x4c, 0x3e])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 300\n",
    "params = TrainingParams(vocab_size, (vocab_size - 256), \"<NULL>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f06f53c-8c64-4dbf-aee2-55e371f2f734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x65], UInt8[0x20]) into a new token UInt8[0x65, 0x20]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x20], UInt8[0x74]) into a new token UInt8[0x20, 0x74]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x69], UInt8[0x6e]) into a new token UInt8[0x69, 0x6e]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x74], UInt8[0x20]) into a new token UInt8[0x74, 0x20]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x73], UInt8[0x20]) into a new token UInt8[0x73, 0x20]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x20, 0x74], UInt8[0x68]) into a new token UInt8[0x20, 0x74, 0x68]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x69], UInt8[0x74]) into a new token UInt8[0x69, 0x74]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x69, 0x6e], UInt8[0x67]) into a new token UInt8[0x69, 0x6e, 0x67]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x65], UInt8[0x61]) into a new token UInt8[0x65, 0x61]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x6f], UInt8[0x75]) into a new token UInt8[0x6f, 0x75]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x6f], UInt8[0x20]) into a new token UInt8[0x6f, 0x20]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x65], UInt8[0x72]) into a new token UInt8[0x65, 0x72]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x61], UInt8[0x6e]) into a new token UInt8[0x61, 0x6e]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x69, 0x6e, 0x67], UInt8[0x20]) into a new token UInt8[0x69, 0x6e, 0x67, 0x20]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x65], UInt8[0x6e]) into a new token UInt8[0x65, 0x6e]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x2e], UInt8[0x20]) into a new token UInt8[0x2e, 0x20]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x79], UInt8[0x20]) into a new token UInt8[0x79, 0x20]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x6f], UInt8[0x72]) into a new token UInt8[0x6f, 0x72]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x77], UInt8[0x72]) into a new token UInt8[0x77, 0x72]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x77, 0x72], UInt8[0x69, 0x74]) into a new token UInt8[0x77, 0x72, 0x69, 0x74]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x64], UInt8[0x20]) into a new token UInt8[0x64, 0x20]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x79], UInt8[0x6f, 0x75]) into a new token UInt8[0x79, 0x6f, 0x75]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x20, 0x74], UInt8[0x6f, 0x20]) into a new token UInt8[0x20, 0x74, 0x6f, 0x20]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x74], UInt8[0x68]) into a new token UInt8[0x74, 0x68]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x2c], UInt8[0x20]) into a new token UInt8[0x2c, 0x20]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x20, 0x74, 0x68], UInt8[0x65, 0x20]) into a new token UInt8[0x20, 0x74, 0x68, 0x65, 0x20]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x70], UInt8[0x6c]) into a new token UInt8[0x70, 0x6c]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x6f], UInt8[0x6e]) into a new token UInt8[0x6f, 0x6e]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x61], UInt8[0x6c]) into a new token UInt8[0x61, 0x6c]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x0a], UInt8[0x0a]) into a new token UInt8[0x0a, 0x0a]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x69], UInt8[0x6d]) into a new token UInt8[0x69, 0x6d]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x6f], UInt8[0x66]) into a new token UInt8[0x6f, 0x66]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x72], UInt8[0x65, 0x61]) into a new token UInt8[0x72, 0x65, 0x61]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x2e], UInt8[0x0a, 0x0a]) into a new token UInt8[0x2e, 0x0a, 0x0a]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x77, 0x72, 0x69, 0x74], UInt8[0x69, 0x6e, 0x67, 0x20]) into a new token UInt8[0x77, 0x72, 0x69, 0x74, 0x69, 0x6e, 0x67, 0x20]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x20], UInt8[0x63]) into a new token UInt8[0x20, 0x63]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x61], UInt8[0x72]) into a new token UInt8[0x61, 0x72]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x65], UInt8[0x73]) into a new token UInt8[0x65, 0x73]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x54], UInt8[0x68]) into a new token UInt8[0x54, 0x68]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x61], UInt8[0x74, 0x20]) into a new token UInt8[0x61, 0x74, 0x20]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x72, 0x65, 0x61], UInt8[0x64]) into a new token UInt8[0x72, 0x65, 0x61, 0x64]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x65], UInt8[0x6d]) into a new token UInt8[0x65, 0x6d]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x20], UInt8[0x73]) into a new token UInt8[0x20, 0x73]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMerging (UInt8[0x69], UInt8[0x6c]) into a new token UInt8[0x69, 0x6c]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict{Int64, Base.CodeUnits{UInt8, String}} with 300 entries:\n",
       "  0  => [0x00]\n",
       "  1  => [0x01]\n",
       "  2  => [0x02]\n",
       "  3  => [0x03]\n",
       "  4  => [0x04]\n",
       "  5  => [0x05]\n",
       "  6  => [0x06]\n",
       "  7  => [0x07]\n",
       "  8  => [0x08]\n",
       "  9  => [0x09]\n",
       "  10 => [0x0a]\n",
       "  11 => [0x0b]\n",
       "  12 => [0x0c]\n",
       "  13 => [0x0d]\n",
       "  14 => [0x0e]\n",
       "  15 => [0x0f]\n",
       "  16 => [0x10]\n",
       "  17 => [0x11]\n",
       "  18 => [0x12]\n",
       "  19 => [0x13]\n",
       "  20 => [0x14]\n",
       "  21 => [0x15]\n",
       "  22 => [0x16]\n",
       "  23 => [0x17]\n",
       "  24 => [0x18]\n",
       "  â®  => â®"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(tokenizer, params, enc_vec) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23f00533-e287-4b68-a765-1cde142a6c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Base.CodeUnits{UInt8, String}:\n",
       " 0x74\n",
       " 0x20"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocabDict[260]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74d1812-98f0-47d1-93f2-32b09b41aa2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
