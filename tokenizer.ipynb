{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca1ca345-7dbe-4e4f-90c0-752374785651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "import regex\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3bb489b-57a0-450d-8181-af70ecb5ee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_without_merging(string_input: str):\n",
    "    return list(string_input.encode(\"utf-8\"))\n",
    "\n",
    "\n",
    "def encode(string_input: str, merges: Dict[Tuple, int]):\n",
    "    tokens = list(string_input.encode(\"utf-8\"))\n",
    "    while len(tokens) >= 2:\n",
    "        pair_cnts = get_pair_cnt(tokens)\n",
    "        pair = min(pair_cnts, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        if pair not in merges:\n",
    "            break\n",
    "        idx = merges[pair]\n",
    "        print(\"pair\", pair)\n",
    "        print(\"index\", idx)\n",
    "        tokens = mint_new_token(tokens, pair, idx)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def build_vocab(merge_dict: Dict) -> Dict[int, str]:\n",
    "    vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "    for (p0, p1), idx in merge_dict.items():\n",
    "        vocab[idx] = vocab[p0] + vocab[p1]\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def decode(ids: int, vocab: Dict[int, str]):\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_pair_cnt(ids: List[int]):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def get_top_pair(ids: List[int]) -> Tuple[int, ...]:\n",
    "    pair_cnts = get_pair_cnt(ids)\n",
    "    top_pair = max(pair_cnts, key=pair_cnts.get)\n",
    "    return top_pair\n",
    "\n",
    "\n",
    "def mint_new_token(ids: List[int], pair: Tuple[int, ...], idx: int):\n",
    "    newIds = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
    "            newIds.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newIds.append(ids[i])\n",
    "            i += 1\n",
    "    return newIds\n",
    "\n",
    "\n",
    "def get_compression_ratio(tokens: List[int], ids: List[int]):\n",
    "    print(\"length of the tokens:\", len(tokens))\n",
    "    print(\"length of the ids:\", len(ids))\n",
    "    print(\"compression ratio:\", round(len(tokens) / len(ids), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ecd5793-0f14-4901-b851-f0e5c30e1bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_str: str = \"hi hello 😆, how are you doing ? お元気ですか \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "900a1a7b-9ceb-4995-8dc9-8e94ea5e1630",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_string = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9552eb3-a583-47ad-a272-2ab18288728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_tokens = encode_without_merging(paragraph_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "080f7300-e6d1-4875-9dbb-7f4737945fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_string_tokens = encode_without_merging(random_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d08cd61d-9272-42e5-94cf-b622f8e88f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the vocab size\n",
    "vocab_size = 300\n",
    "n_merges = vocab_size - 256\n",
    "# copy the tokens list\n",
    "ids = list(paragraph_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c606d67a-88af-424e-b8a9-ef84d6af5de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into a new token 256\n",
      "merging (240, 159) into a new token 257\n",
      "merging (226, 128) into a new token 258\n",
      "merging (105, 110) into a new token 259\n",
      "merging (115, 32) into a new token 260\n",
      "merging (97, 110) into a new token 261\n",
      "merging (116, 104) into a new token 262\n",
      "merging (257, 133) into a new token 263\n",
      "merging (257, 135) into a new token 264\n",
      "merging (97, 114) into a new token 265\n",
      "merging (239, 189) into a new token 266\n",
      "merging (258, 140) into a new token 267\n",
      "merging (267, 264) into a new token 268\n",
      "merging (101, 114) into a new token 269\n",
      "merging (111, 114) into a new token 270\n",
      "merging (116, 32) into a new token 271\n",
      "merging (259, 103) into a new token 272\n",
      "merging (115, 116) into a new token 273\n",
      "merging (261, 100) into a new token 274\n",
      "merging (32, 262) into a new token 275\n",
      "merging (44, 32) into a new token 276\n",
      "merging (97, 109) into a new token 277\n",
      "merging (275, 256) into a new token 278\n",
      "merging (111, 117) into a new token 279\n",
      "merging (85, 110) into a new token 280\n",
      "merging (280, 105) into a new token 281\n",
      "merging (281, 99) into a new token 282\n",
      "merging (282, 111) into a new token 283\n",
      "merging (283, 100) into a new token 284\n",
      "merging (115, 276) into a new token 285\n",
      "merging (273, 114) into a new token 286\n",
      "merging (101, 265) into a new token 287\n",
      "merging (274, 32) into a new token 288\n",
      "merging (259, 116) into a new token 289\n",
      "merging (111, 102) into a new token 290\n",
      "merging (46, 32) into a new token 291\n",
      "merging (108, 108) into a new token 292\n",
      "merging (272, 32) into a new token 293\n",
      "merging (261, 32) into a new token 294\n",
      "merging (101, 110) into a new token 295\n",
      "merging (33, 32) into a new token 296\n",
      "merging (118, 269) into a new token 297\n",
      "merging (121, 32) into a new token 298\n",
      "merging (277, 256) into a new token 299\n"
     ]
    }
   ],
   "source": [
    "merges = {}\n",
    "for i in range(n_merges):\n",
    "    top_pair = get_top_pair(ids)\n",
    "    idx = 256 + i\n",
    "    print(f\"merging {top_pair} into a new token {idx}\")\n",
    "    ids = mint_new_token(ids, top_pair, idx)\n",
    "    merges[top_pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3229d9c-e458-433e-969c-7a8c508cb74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(101, 32): 256,\n",
       " (240, 159): 257,\n",
       " (226, 128): 258,\n",
       " (105, 110): 259,\n",
       " (115, 32): 260,\n",
       " (97, 110): 261,\n",
       " (116, 104): 262,\n",
       " (257, 133): 263,\n",
       " (257, 135): 264,\n",
       " (97, 114): 265,\n",
       " (239, 189): 266,\n",
       " (258, 140): 267,\n",
       " (267, 264): 268,\n",
       " (101, 114): 269,\n",
       " (111, 114): 270,\n",
       " (116, 32): 271,\n",
       " (259, 103): 272,\n",
       " (115, 116): 273,\n",
       " (261, 100): 274,\n",
       " (32, 262): 275,\n",
       " (44, 32): 276,\n",
       " (97, 109): 277,\n",
       " (275, 256): 278,\n",
       " (111, 117): 279,\n",
       " (85, 110): 280,\n",
       " (280, 105): 281,\n",
       " (281, 99): 282,\n",
       " (282, 111): 283,\n",
       " (283, 100): 284,\n",
       " (115, 276): 285,\n",
       " (273, 114): 286,\n",
       " (101, 265): 287,\n",
       " (274, 32): 288,\n",
       " (259, 116): 289,\n",
       " (111, 102): 290,\n",
       " (46, 32): 291,\n",
       " (108, 108): 292,\n",
       " (272, 32): 293,\n",
       " (261, 32): 294,\n",
       " (101, 110): 295,\n",
       " (33, 32): 296,\n",
       " (118, 269): 297,\n",
       " (121, 32): 298,\n",
       " (277, 256): 299}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ca54e51-80bc-4726-8e85-aedae49b854f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the tokens: 616\n",
      "length of the ids: 372\n",
      "compression ratio: 1.66\n"
     ]
    }
   ],
   "source": [
    "get_compression_ratio(paragraph_tokens, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc31707f-2df3-4166-b25e-b2995b45b5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair (101, 32)\n",
      "index 256\n",
      "pair (240, 159)\n",
      "index 257\n",
      "pair (105, 110)\n",
      "index 259\n",
      "pair (97, 114)\n",
      "index 265\n",
      "pair (259, 103)\n",
      "index 272\n",
      "pair (44, 32)\n",
      "index 276\n",
      "pair (111, 117)\n",
      "index 279\n",
      "pair (108, 108)\n",
      "index 292\n",
      "pair (272, 32)\n",
      "index 293\n"
     ]
    }
   ],
   "source": [
    "encoded_random_str = encode(random_str, merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6ae1989-37df-4a53-89d3-97b7b816dfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ae3f2d6-0e7c-4207-8a66-6afbc3e220ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'\\x00',\n",
       " 1: b'\\x01',\n",
       " 2: b'\\x02',\n",
       " 3: b'\\x03',\n",
       " 4: b'\\x04',\n",
       " 5: b'\\x05',\n",
       " 6: b'\\x06',\n",
       " 7: b'\\x07',\n",
       " 8: b'\\x08',\n",
       " 9: b'\\t',\n",
       " 10: b'\\n',\n",
       " 11: b'\\x0b',\n",
       " 12: b'\\x0c',\n",
       " 13: b'\\r',\n",
       " 14: b'\\x0e',\n",
       " 15: b'\\x0f',\n",
       " 16: b'\\x10',\n",
       " 17: b'\\x11',\n",
       " 18: b'\\x12',\n",
       " 19: b'\\x13',\n",
       " 20: b'\\x14',\n",
       " 21: b'\\x15',\n",
       " 22: b'\\x16',\n",
       " 23: b'\\x17',\n",
       " 24: b'\\x18',\n",
       " 25: b'\\x19',\n",
       " 26: b'\\x1a',\n",
       " 27: b'\\x1b',\n",
       " 28: b'\\x1c',\n",
       " 29: b'\\x1d',\n",
       " 30: b'\\x1e',\n",
       " 31: b'\\x1f',\n",
       " 32: b' ',\n",
       " 33: b'!',\n",
       " 34: b'\"',\n",
       " 35: b'#',\n",
       " 36: b'$',\n",
       " 37: b'%',\n",
       " 38: b'&',\n",
       " 39: b\"'\",\n",
       " 40: b'(',\n",
       " 41: b')',\n",
       " 42: b'*',\n",
       " 43: b'+',\n",
       " 44: b',',\n",
       " 45: b'-',\n",
       " 46: b'.',\n",
       " 47: b'/',\n",
       " 48: b'0',\n",
       " 49: b'1',\n",
       " 50: b'2',\n",
       " 51: b'3',\n",
       " 52: b'4',\n",
       " 53: b'5',\n",
       " 54: b'6',\n",
       " 55: b'7',\n",
       " 56: b'8',\n",
       " 57: b'9',\n",
       " 58: b':',\n",
       " 59: b';',\n",
       " 60: b'<',\n",
       " 61: b'=',\n",
       " 62: b'>',\n",
       " 63: b'?',\n",
       " 64: b'@',\n",
       " 65: b'A',\n",
       " 66: b'B',\n",
       " 67: b'C',\n",
       " 68: b'D',\n",
       " 69: b'E',\n",
       " 70: b'F',\n",
       " 71: b'G',\n",
       " 72: b'H',\n",
       " 73: b'I',\n",
       " 74: b'J',\n",
       " 75: b'K',\n",
       " 76: b'L',\n",
       " 77: b'M',\n",
       " 78: b'N',\n",
       " 79: b'O',\n",
       " 80: b'P',\n",
       " 81: b'Q',\n",
       " 82: b'R',\n",
       " 83: b'S',\n",
       " 84: b'T',\n",
       " 85: b'U',\n",
       " 86: b'V',\n",
       " 87: b'W',\n",
       " 88: b'X',\n",
       " 89: b'Y',\n",
       " 90: b'Z',\n",
       " 91: b'[',\n",
       " 92: b'\\\\',\n",
       " 93: b']',\n",
       " 94: b'^',\n",
       " 95: b'_',\n",
       " 96: b'`',\n",
       " 97: b'a',\n",
       " 98: b'b',\n",
       " 99: b'c',\n",
       " 100: b'd',\n",
       " 101: b'e',\n",
       " 102: b'f',\n",
       " 103: b'g',\n",
       " 104: b'h',\n",
       " 105: b'i',\n",
       " 106: b'j',\n",
       " 107: b'k',\n",
       " 108: b'l',\n",
       " 109: b'm',\n",
       " 110: b'n',\n",
       " 111: b'o',\n",
       " 112: b'p',\n",
       " 113: b'q',\n",
       " 114: b'r',\n",
       " 115: b's',\n",
       " 116: b't',\n",
       " 117: b'u',\n",
       " 118: b'v',\n",
       " 119: b'w',\n",
       " 120: b'x',\n",
       " 121: b'y',\n",
       " 122: b'z',\n",
       " 123: b'{',\n",
       " 124: b'|',\n",
       " 125: b'}',\n",
       " 126: b'~',\n",
       " 127: b'\\x7f',\n",
       " 128: b'\\x80',\n",
       " 129: b'\\x81',\n",
       " 130: b'\\x82',\n",
       " 131: b'\\x83',\n",
       " 132: b'\\x84',\n",
       " 133: b'\\x85',\n",
       " 134: b'\\x86',\n",
       " 135: b'\\x87',\n",
       " 136: b'\\x88',\n",
       " 137: b'\\x89',\n",
       " 138: b'\\x8a',\n",
       " 139: b'\\x8b',\n",
       " 140: b'\\x8c',\n",
       " 141: b'\\x8d',\n",
       " 142: b'\\x8e',\n",
       " 143: b'\\x8f',\n",
       " 144: b'\\x90',\n",
       " 145: b'\\x91',\n",
       " 146: b'\\x92',\n",
       " 147: b'\\x93',\n",
       " 148: b'\\x94',\n",
       " 149: b'\\x95',\n",
       " 150: b'\\x96',\n",
       " 151: b'\\x97',\n",
       " 152: b'\\x98',\n",
       " 153: b'\\x99',\n",
       " 154: b'\\x9a',\n",
       " 155: b'\\x9b',\n",
       " 156: b'\\x9c',\n",
       " 157: b'\\x9d',\n",
       " 158: b'\\x9e',\n",
       " 159: b'\\x9f',\n",
       " 160: b'\\xa0',\n",
       " 161: b'\\xa1',\n",
       " 162: b'\\xa2',\n",
       " 163: b'\\xa3',\n",
       " 164: b'\\xa4',\n",
       " 165: b'\\xa5',\n",
       " 166: b'\\xa6',\n",
       " 167: b'\\xa7',\n",
       " 168: b'\\xa8',\n",
       " 169: b'\\xa9',\n",
       " 170: b'\\xaa',\n",
       " 171: b'\\xab',\n",
       " 172: b'\\xac',\n",
       " 173: b'\\xad',\n",
       " 174: b'\\xae',\n",
       " 175: b'\\xaf',\n",
       " 176: b'\\xb0',\n",
       " 177: b'\\xb1',\n",
       " 178: b'\\xb2',\n",
       " 179: b'\\xb3',\n",
       " 180: b'\\xb4',\n",
       " 181: b'\\xb5',\n",
       " 182: b'\\xb6',\n",
       " 183: b'\\xb7',\n",
       " 184: b'\\xb8',\n",
       " 185: b'\\xb9',\n",
       " 186: b'\\xba',\n",
       " 187: b'\\xbb',\n",
       " 188: b'\\xbc',\n",
       " 189: b'\\xbd',\n",
       " 190: b'\\xbe',\n",
       " 191: b'\\xbf',\n",
       " 192: b'\\xc0',\n",
       " 193: b'\\xc1',\n",
       " 194: b'\\xc2',\n",
       " 195: b'\\xc3',\n",
       " 196: b'\\xc4',\n",
       " 197: b'\\xc5',\n",
       " 198: b'\\xc6',\n",
       " 199: b'\\xc7',\n",
       " 200: b'\\xc8',\n",
       " 201: b'\\xc9',\n",
       " 202: b'\\xca',\n",
       " 203: b'\\xcb',\n",
       " 204: b'\\xcc',\n",
       " 205: b'\\xcd',\n",
       " 206: b'\\xce',\n",
       " 207: b'\\xcf',\n",
       " 208: b'\\xd0',\n",
       " 209: b'\\xd1',\n",
       " 210: b'\\xd2',\n",
       " 211: b'\\xd3',\n",
       " 212: b'\\xd4',\n",
       " 213: b'\\xd5',\n",
       " 214: b'\\xd6',\n",
       " 215: b'\\xd7',\n",
       " 216: b'\\xd8',\n",
       " 217: b'\\xd9',\n",
       " 218: b'\\xda',\n",
       " 219: b'\\xdb',\n",
       " 220: b'\\xdc',\n",
       " 221: b'\\xdd',\n",
       " 222: b'\\xde',\n",
       " 223: b'\\xdf',\n",
       " 224: b'\\xe0',\n",
       " 225: b'\\xe1',\n",
       " 226: b'\\xe2',\n",
       " 227: b'\\xe3',\n",
       " 228: b'\\xe4',\n",
       " 229: b'\\xe5',\n",
       " 230: b'\\xe6',\n",
       " 231: b'\\xe7',\n",
       " 232: b'\\xe8',\n",
       " 233: b'\\xe9',\n",
       " 234: b'\\xea',\n",
       " 235: b'\\xeb',\n",
       " 236: b'\\xec',\n",
       " 237: b'\\xed',\n",
       " 238: b'\\xee',\n",
       " 239: b'\\xef',\n",
       " 240: b'\\xf0',\n",
       " 241: b'\\xf1',\n",
       " 242: b'\\xf2',\n",
       " 243: b'\\xf3',\n",
       " 244: b'\\xf4',\n",
       " 245: b'\\xf5',\n",
       " 246: b'\\xf6',\n",
       " 247: b'\\xf7',\n",
       " 248: b'\\xf8',\n",
       " 249: b'\\xf9',\n",
       " 250: b'\\xfa',\n",
       " 251: b'\\xfb',\n",
       " 252: b'\\xfc',\n",
       " 253: b'\\xfd',\n",
       " 254: b'\\xfe',\n",
       " 255: b'\\xff',\n",
       " 256: b'e ',\n",
       " 257: b'\\xf0\\x9f',\n",
       " 258: b'\\xe2\\x80',\n",
       " 259: b'in',\n",
       " 260: b's ',\n",
       " 261: b'an',\n",
       " 262: b'th',\n",
       " 263: b'\\xf0\\x9f\\x85',\n",
       " 264: b'\\xf0\\x9f\\x87',\n",
       " 265: b'ar',\n",
       " 266: b'\\xef\\xbd',\n",
       " 267: b'\\xe2\\x80\\x8c',\n",
       " 268: b'\\xe2\\x80\\x8c\\xf0\\x9f\\x87',\n",
       " 269: b'er',\n",
       " 270: b'or',\n",
       " 271: b't ',\n",
       " 272: b'ing',\n",
       " 273: b'st',\n",
       " 274: b'and',\n",
       " 275: b' th',\n",
       " 276: b', ',\n",
       " 277: b'am',\n",
       " 278: b' the ',\n",
       " 279: b'ou',\n",
       " 280: b'Un',\n",
       " 281: b'Uni',\n",
       " 282: b'Unic',\n",
       " 283: b'Unico',\n",
       " 284: b'Unicod',\n",
       " 285: b's, ',\n",
       " 286: b'str',\n",
       " 287: b'ear',\n",
       " 288: b'and ',\n",
       " 289: b'int',\n",
       " 290: b'of',\n",
       " 291: b'. ',\n",
       " 292: b'll',\n",
       " 293: b'ing ',\n",
       " 294: b'an ',\n",
       " 295: b'en',\n",
       " 296: b'! ',\n",
       " 297: b'ver',\n",
       " 298: b'y ',\n",
       " 299: b'ame '}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6d24607-8989-40ea-896b-7f433a7710d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi hello 😆, how are you doing ? お元気ですか \n"
     ]
    }
   ],
   "source": [
    "print(decode(encoded_random_str, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed428cf1-6ae7-42ae-ad74-72fd7e208d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2pat = regex.compile(\n",
    "    r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25fd466e-ef60-4960-beae-26a680aa90e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hellow', ' how', \"'ve\", ' are', ' you', '?', '123', ' お元', ' 気', ' ですか']\n"
     ]
    }
   ],
   "source": [
    "print(regex.findall(gpt2pat, \"Hellow how've are you?123 お元 気 ですか\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d36b6c46-4fbc-4ef9-9039-aecf10ee94cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b2020e5-bb0b-4dd7-b47e-3bd10321a3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71, 5412, 1268, 527, 577, 9602]\n"
     ]
    }
   ],
   "source": [
    "print(enc.encode(\"hellow how are u ??\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b18241e-a688-4e4d-b005-9fcee63de2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
